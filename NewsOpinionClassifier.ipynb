{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an SVM classifier of News and Opinion pieces based on lemma, pos tag, and linguistic features\n",
    "\n",
    "#### Based on year 1987 of NYTAC corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import glob, os\n",
    "\n",
    "i = 0\n",
    "\n",
    "DocStrings = []\n",
    "\n",
    "data_dir = 'data\\\\1987\\\\1987\\\\**' # Change to path on your machine\n",
    "\n",
    "for filename in glob.iglob(data_dir, recursive=True):\n",
    "    if os.path.isfile(filename) and filename.endswith('.xml'):\n",
    "        print(i, filename)\n",
    "        i += 1\n",
    "        f = open(filename, \"r\")\n",
    "        text = f.read()\n",
    "        DocStrings.append((filename, text))\n",
    "\n",
    "\n",
    "Texts = [] # A list of the main texts of documents in the corpus\n",
    "\n",
    "multiples = []\n",
    "empties = []\n",
    "i = 0\n",
    "for doc in DocStrings:\n",
    "    print(i)\n",
    "    i += 1\n",
    "    file = doc[1]\n",
    "    soup = BeautifulSoup(file, 'lxml-xml')\n",
    "    l = soup.find_all('block', class_='full_text')\n",
    "    if len(l) == 0:\n",
    "        empties.append(doc[0])\n",
    "    if len(l) > 1:\n",
    "        multiples.append(doc[0])\n",
    "    Texts.append((doc[0], ' '.join([b.text for b in l])))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"DocStrings.pckl\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(DocStrings, fp)\n",
    "    \n",
    "with open(\"Texts.pckl\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(Texts, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category(tax_list):\n",
    "    \n",
    "    # Method that takes a list of taxonomical classifiers and outputs two boolean values for being News and being Opinion\n",
    "    \n",
    "    nw = any([t.string.startswith('Top/News') for t in tax_list])\n",
    "    op = any([t.string.startswith('Top/Opinion') for t in tax_list])\n",
    "    return tuple((nw, op))\n",
    "\n",
    "def categorize(filename):\n",
    "    \n",
    "    # Method that takes a filename and outputs whether it is News and whether it is Opinion\n",
    "    \n",
    "    with open(filename) as fp:\n",
    "        soup = BeautifulSoup(fp, 'lxml-xml')\n",
    "    l = soup.find_all('classifier', type='taxonomic_classifier')\n",
    "    return category(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "i = 0\n",
    "\n",
    "categorized = []  # A list of categories of the documents\n",
    "\n",
    "news_pieces = 0\n",
    "opinion_pieces = 0\n",
    "\n",
    "data_dir = 'data\\\\1987\\\\1987\\\\**' # Change to path on your machine\n",
    "\n",
    "for filename in glob.iglob(data_dir, recursive=True):\n",
    "    if os.path.isfile(filename) and filename.endswith('.xml'):\n",
    "        print(i, filename)\n",
    "        i += 1\n",
    "        categorized.append((filename, categorize(filename)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"categorized.pckl\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(categorized, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Now we begin parsing the texts using the \"Stanza\" package from Stanford NLP \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "stanza.download(\"en\")\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,mwt,pos,lemma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Store a list of the output of Stanza on parsing each of the texts in the corpus\n",
    "\n",
    "StanzaParses = []\n",
    "i = 0\n",
    "t0 = time.time()\n",
    "for text in Texts[n:]:\n",
    "    \n",
    "    i += 1\n",
    "    \n",
    "    #### This try - except is where about five defective documents appeared that Stanza could not parse\n",
    "    #### They will be removed further down below\n",
    "    \n",
    "    try:\n",
    "        StanzaParses.append((text[0], nlp(text[1])))\n",
    "        print(i, (time.time()-t0)/i, text[0])\n",
    "    except:\n",
    "        StanzaParses.append((text[0], None))\n",
    "        print(i, (time.time()-t0)/i, text[0], ' exception')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Change the format to lists of dictionaries for convenient writing to and reading from files\n",
    "\n",
    "### From now on tokens are understood to be dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "        \n",
    "StanzaDicts = []\n",
    "i = 0\n",
    "t0 = time.time()\n",
    "for tup in StanzaParses:\n",
    "    if tup[1]!=None:\n",
    "        StanzaDicts.append((tup[0], [sentence.to_dict() for sentence in tup[1].sentences]))\n",
    "    else:\n",
    "        StanzaDicts.append((tup[0], None))\n",
    "        \n",
    "    i += 1\n",
    "    print(i, (time.time()-t0)/i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"StanzaDicts.pckl\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(StanzaDicts, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this part we devise a strategy to separate segments within quotations from text outside of quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quotIndices(sentence):\n",
    "    \n",
    "    # This method takes a sentence (as defined by Stanza's parse) and identifies all indices with opening or closing quotations\n",
    "    \n",
    "    inds = []\n",
    "    for k in range(len(sentence)):\n",
    "        word = sentence[k][\"text\"]\n",
    "        if word.startswith(\"''\") or word.startswith('\"'):\n",
    "            inds.append(k)\n",
    "        elif (word.endswith(\"'\")) and k+1 < len(sentence) and sentence[k+1][\"text\"].startswith(\"'\"):\n",
    "            inds.append(k+1)\n",
    "        elif word.endswith('\"'):\n",
    "            inds.append(k+1)\n",
    "        elif '\"' in word or \"''\" in word:\n",
    "            inds.append(k)\n",
    "    return sorted(set(inds))\n",
    "\n",
    "def quotations(sentence):\n",
    "    \n",
    "    # This method outputs a list of tokens outside of quotes and a list of tokens within quotes\n",
    "    \n",
    "    inds = quotIndices(sentence)\n",
    "    unquoted = []\n",
    "    quoted = []\n",
    "    if len(inds)>0:\n",
    "        unquoted += sentence[:inds[0]]\n",
    "    else:\n",
    "        unquoted = sentence\n",
    "    for i in range(len(inds)):\n",
    "        if i % 2 == 0:\n",
    "            if i+1 < len(inds):\n",
    "                quoted += sentence[inds[i]:inds[i+1]]\n",
    "            else:\n",
    "                quoted += sentence[inds[i]:]\n",
    "        elif i % 2 == 1:\n",
    "            if i+1 < len(inds):\n",
    "                unquoted += sentence[inds[i]:inds[i+1]]\n",
    "            else:\n",
    "                unquoted += sentence[inds[i]:]\n",
    "        else:\n",
    "            raise Exception(\"There is no other mathematical possibility!\")\n",
    "    return (unquoted, quoted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pair of lists of tokens outside and withing quotations for each sentence in a list of sentences for each document\n",
    "# for a list of documents for the entire corpus\n",
    "\n",
    "Quotes = []\n",
    "t0 = time.time()\n",
    "for i in range(len(StanzaDicts)):\n",
    "    print(i, (time.time()-t0)/(i+1))\n",
    "    SentenceQuotes = []\n",
    "    if StanzaDicts[i][1] != None:\n",
    "        for j in range(len(StanzaDicts[i][1])):\n",
    "            SentenceQuotes.append(quotations(StanzaDicts[i][1][j]))\n",
    "    else:\n",
    "        SentenceQuotes = None\n",
    "    Quotes.append((StanzaDicts[i][0], SentenceQuotes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Quotes.pckl\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(Quotes, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We begin compiling a list of the value of each document for each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NormNegation = []\n",
    "for doc in StanzaDicts:\n",
    "    if doc[1] != None:\n",
    "        NormNegation.append(len([tok for sent in doc[1] for tok in sent if tok[\"lemma\"] in NegationWords])/len(doc[1]))\n",
    "    else:\n",
    "        NormNegation.append(None)\n",
    "        \n",
    "NegationRatio = []\n",
    "for doc in StanzaDicts:\n",
    "    if doc[1] == None:\n",
    "        NegationRatio.append(None)\n",
    "    else:\n",
    "        NegationRatio.append(len([tok for sent in doc[1] for tok in sent if tok[\"lemma\"] in NegationWords])/sum([len(sent) for sent in doc[1]]))\n",
    "\n",
    "NormNegSuffix = []\n",
    "for doc in StanzaDicts:\n",
    "    if doc[1] == None:\n",
    "        NormNegSuffix.append(None)\n",
    "    else:\n",
    "        NormNegSuffix.append(len([tok for sent in doc[1] for tok in sent if tok[\"text\"] == \"n't\"])/len(doc[1]))\n",
    "        \n",
    "NegSuffixRatio = []\n",
    "for doc in StanzaDicts:\n",
    "    if doc[1] == None:\n",
    "        NegSuffixRatio.append(None)\n",
    "    else:\n",
    "        NegSuffixRatio.append(len([tok for sent in doc[1] for tok in sent if tok[\"text\"] == \"n't\"])/sum([len(sent) for sent in doc[1]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feats(token):\n",
    "    \n",
    "    # This method returns a dictionary of the \"feats\" of a token if there are any\n",
    "    \n",
    "    if \"feats\" in token:\n",
    "        s = token[\"feats\"]\n",
    "        l = s.split('|')\n",
    "        d = {f.split('=')[0] : f.split('=')[1] for f in l }\n",
    "        return d\n",
    "    else:\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FinVerb(token):\n",
    "    \n",
    "    # Returns whether a token is a finite verb\n",
    "    \n",
    "    fts = feats(token)\n",
    "    return 'VerbForm' in fts and fts[\"VerbForm\"] == 'Fin'\n",
    "\n",
    "Complexity = []\n",
    "for doc in StanzaDicts:\n",
    "    if doc[1] == None:\n",
    "        Complexity.append(None)\n",
    "    else:\n",
    "        Complexity.append(len([tok for sent in doc[1] for tok in sent if FinVerb(tok)])/len(doc[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Questions = []\n",
    "for doc in StanzaDicts:\n",
    "    if doc[1] == None:\n",
    "        Questions.append(None)\n",
    "    else:\n",
    "        NumEndPuncts = len([tok for sent in doc[1] for tok in sent if tok[\"text\"] in {'.', '?', '!'}])\n",
    "        if NumEndPuncts == 0:\n",
    "            Questions.append(0)\n",
    "        else:\n",
    "            Questions.append(len([tok for sent in doc[1] for tok in sent if tok[\"text\"] == '?'])/NumEndPuncts)\n",
    "            \n",
    "Exclamations = []\n",
    "for doc in StanzaDicts:\n",
    "    if doc[1] == None:\n",
    "        Questions.append(None)\n",
    "    else:\n",
    "        NumEndPuncts = len([tok for sent in doc[1] for tok in sent if tok[\"text\"] in {'.', '?', '!'}])\n",
    "        if NumEndPuncts == 0:\n",
    "            Exclamations.append(0)\n",
    "        else:\n",
    "            Exclamations.append(len([tok for sent in doc[1] for tok in sent if tok[\"text\"] == '!'])/NumEndPuncts)\n",
    "\n",
    "Semicolons = []\n",
    "for doc in StanzaDicts:\n",
    "    if doc[1] == None:\n",
    "        Semicolons.append(None)\n",
    "    else:\n",
    "        NumPuncts = len([tok for sent in doc[1] for tok in sent if tok[\"text\"] in {'.', '?', '!', ';', ','}])\n",
    "        if NumPuncts == 0:\n",
    "            Semicolons.append(0)\n",
    "        else:\n",
    "            Semicolons.append(len([tok for sent in doc[1] for tok in sent if tok[\"text\"] == ';'])/NumPuncts)\n",
    "            \n",
    "Commas = []\n",
    "for doc in StanzaDicts:\n",
    "    if doc[1] == None:\n",
    "        Commas.append(None)\n",
    "    else:\n",
    "        NumPuncts = len([tok for sent in doc[1] for tok in sent if tok[\"text\"] in {'.', '?', '!', ';', ','}])\n",
    "        if NumPuncts == 0:\n",
    "            Commas.append(0)\n",
    "        else:\n",
    "            Commas.append(len([tok for sent in doc[1] for tok in sent if tok[\"text\"] == ','])/NumPuncts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matches(text, phrase):\n",
    "    \n",
    "    # Returns number of times a phrase appears in a given text\n",
    "    \n",
    "    n = len(text)\n",
    "    i = 0\n",
    "    k = 0\n",
    "    while i < n:\n",
    "        if phrase in text[i:]:\n",
    "            j = text[i:].index(phrase)\n",
    "        else:\n",
    "            i = n+1\n",
    "            return k\n",
    "        i = i + j + len(phrase)\n",
    "        k += 1\n",
    "        if i >= n:\n",
    "            return k\n",
    "    return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCONJcausal = ['since', 'as', 'though', 'that']\n",
    "CCONJcausal = ['yet']\n",
    "causal = ['accordingly', 'because', 'hence', 'thus', 'therefore', 'consequently']\n",
    "MULTIcausal = ['as a result', 'stemming from this', 'as an effect', 'in that case']\n",
    "\n",
    "def SentCausals(sent):\n",
    "    # Returns number of causal connectives in a sentence\n",
    "    sconjs = len([tok for tok in sent if tok[\"upos\"] == 'SCONJ' and tok[\"lemma\"] in SCONJcausal])\n",
    "    cconjs = len([tok for tok in sent if tok[\"upos\"] == 'CCONJ' and tok[\"lemma\"] in CCONJcausal])\n",
    "    plains = len([tok for tok in sent if tok[\"lemma\"] in causal])\n",
    "    text = \" \".join([tok[\"lemma\"] for tok in sent])\n",
    "    multis = 0\n",
    "    for phrase in MULTIcausal:\n",
    "        multis += matches(text, phrase)\n",
    "    return sconjs + cconjs + plains + multis\n",
    "\n",
    "Causals = []\n",
    "j = 0\n",
    "for doc in StanzaDicts:\n",
    "    print(j)\n",
    "    j+=1\n",
    "    if doc[1] == None:\n",
    "        Causals.append(None)\n",
    "    else:\n",
    "        Causals.append(sum([SentCausals(sent) for sent in doc[1]])/len(doc[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCONJtemporal = ['before']\n",
    "temporal = ['first', 'lastly', 'finally', 'later', 'meanwhile', 'now', 'previously', 'since', 'straightaway', 'then', 'until', 'when', 'whenever', 'while', 'soon', 'lastly', 'afterwards', 'after']\n",
    "MULTItemporal = ['at once', 'at this moment', 'at this point', 'in the end']\n",
    "\n",
    "def SentTemporals(sent):\n",
    "    # Returns number of temporal connectives in a sentence\n",
    "    sconjs = len([tok for tok in sent if tok[\"upos\"] == 'SCONJ' and tok[\"lemma\"] in SCONJtemporal])\n",
    "    plains = len([tok for tok in sent if tok[\"lemma\"] in temporal])\n",
    "    text = \" \".join([tok[\"lemma\"] for tok in sent])\n",
    "    multis = 0\n",
    "    for phrase in MULTItemporal:\n",
    "        multis += matches(text, phrase)\n",
    "    return sconjs + plains + multis\n",
    "\n",
    "Temporals = []\n",
    "j = 0\n",
    "for doc in StanzaDicts:\n",
    "    print(j)\n",
    "    j+=1\n",
    "    if doc[1] == None:\n",
    "        Temporals.append(None)\n",
    "    else:\n",
    "        Temporals.append(sum([SentTemporals(sent) for sent in doc[1]])/len(doc[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrast = ['alternatively', 'anyway', 'but', 'however', 'instead', 'despite', 'nevertheless', 'although', 'whereas', 'otherwise', 'unlike']\n",
    "MULTIcontrast = ['in spite of', 'on the contrary', 'contrary to', 'by contrast', 'in contrast', 'even so', 'on the other hand', 'even though']\n",
    "\n",
    "def SentContrastives(sent):\n",
    "    # Returns number of contrastive connectives in a sentence\n",
    "    plains = len([tok for tok in sent if tok[\"lemma\"] in contrast])\n",
    "    text = \" \".join([tok[\"lemma\"] for tok in sent])\n",
    "    multis = 0\n",
    "    for phrase in MULTIcontrast:\n",
    "        multis += matches(text, phrase)\n",
    "    return plains + multis\n",
    "\n",
    "Contrastives = []\n",
    "j = 0\n",
    "for doc in StanzaDicts:\n",
    "    print(j)\n",
    "    j+=1\n",
    "    if doc[1] == None:\n",
    "        Contrastives.append(None)\n",
    "    else:\n",
    "        Contrastives.append(sum([SentContrastives(sent) for sent in doc[1]])/len(doc[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expansives = ['additionally', 'also',  'even', 'furthermore',  'indeed',  'moreover', 'besides', 'e.g.', 'i.e.']\n",
    "MULTIexpansives = ['as well', 'in addition', 'let alone', 'not only', 'for example', 'for instance', 'in other words', 'in that', 'that is to say']\n",
    "\n",
    "def SentExpansives(sent):\n",
    "    # Returns number of expansive connectives in a sentence\n",
    "    plains = len([tok for tok in sent if tok[\"lemma\"] in expansives])\n",
    "    text = \" \".join([tok[\"lemma\"] for tok in sent])\n",
    "    multis = 0\n",
    "    for phrase in MULTIexpansives:\n",
    "        multis += matches(text, phrase)\n",
    "    return plains + multis\n",
    "\n",
    "Expansives = []\n",
    "j = 0\n",
    "for doc in StanzaDicts:\n",
    "    print(j)\n",
    "    j+=1\n",
    "    if doc[1] == None:\n",
    "        Expansives.append(None)\n",
    "    else:\n",
    "        Expansives.append(sum([SentExpansives(sent) for sent in doc[1]])/len(doc[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SentConnectives(sent):\n",
    "    # Returns total number of connectives in a sentence\n",
    "    return SentCausals(sent) + SentTemporals(sent) + SentContrastives(sent) + SentExpansives(sent)\n",
    "\n",
    "Connectives = []\n",
    "j = 0\n",
    "for doc in StanzaDicts:\n",
    "    print(j)\n",
    "    j+=1\n",
    "    if doc[1] == None:\n",
    "        Connectives.append(None)\n",
    "    else:\n",
    "        Connectives.append(sum([SentConnectives(sent) for sent in doc[1]])/sum([len(sent) for sent in doc[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Citations = [] #Average number of quotes per sentence\n",
    "j = 0\n",
    "for doc in StanzaDicts:\n",
    "    print(j)\n",
    "    j+=1\n",
    "    if doc[1] == None:\n",
    "        Citations.append(None)\n",
    "    else:\n",
    "        Citations.append(sum([(len(quotIndices(sent))+1)//2 for sent in doc[1]])/len(doc[1]))\n",
    "        \n",
    "CitationLengths = [] #Average number of tokens in each citation for each document\n",
    "j = 0\n",
    "for doc in StanzaDicts:\n",
    "    print(j)\n",
    "    j+=1\n",
    "    if doc[1] == None:\n",
    "        CitationLengths.append(None)\n",
    "    else:\n",
    "        NumCitations = sum([(len(quotIndices(sent))+1)//2 for sent in doc[1]])\n",
    "        if NumCitations == 0:\n",
    "            CitationLengths.append(0)\n",
    "        else:\n",
    "            CitationLengths.append(sum([len(sent[1]) for sent in Quotes[j-1][1]])/NumCitations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PastVerb(token):\n",
    "    # Returns whether token is a past verb\n",
    "    fts = feats(token)\n",
    "    return \"Tense\" in fts and fts[\"Tense\"] == 'Past'\n",
    "\n",
    "Pasts = []\n",
    "j = 0\n",
    "for doc in Quotes:\n",
    "    print(j)\n",
    "    j+=1\n",
    "    if doc[1] == None:\n",
    "        Pasts.append(None)\n",
    "    else:\n",
    "        Pasts.append(len([tok for sent in doc[1] for tok in sent[0] if PastVerb(tok)])/len(doc[1])) #sent[0] unquoted parts of sent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PresentVerb(token):\n",
    "    # Returns whether token is a present verb\n",
    "    fts = feats(token)\n",
    "    return \"Tense\" in fts and fts[\"Tense\"] == 'Pres'\n",
    "\n",
    "Presents = []\n",
    "j = 0\n",
    "for doc in Quotes:\n",
    "    print(j)\n",
    "    j+=1\n",
    "    if doc[1] == None:\n",
    "        Presents.append(None)\n",
    "    else:\n",
    "        Presents.append(len([tok for sent in doc[1] for tok in sent[0] if PresentVerb(tok)])/len(doc[1])) #sent[0] unquoted parts of sent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "communicationVerbs = ['address', 'disseminate', 'profile', 'advertise', 'document', 'proofread', 'advise', 'draft', 'publicize',\n",
    "                      'alert', 'edit', 'publish', 'amend', 'email', 'query', 'announce', 'generate', 'question', 'answer', 'emphasize'\n",
    "                      ,'record', 'arbitrate', 'explain', 'relay', 'articulate', 'express', 'report', 'author', 'frame', 'respond',\n",
    "                      'brand', 'highlight', 'rewrite', 'brief', 'inform', 'scribe', 'broadcast', 'interact', 'chronicle', 'interface',\n",
    "                      'share', 'circulate', 'interpret', 'socialize', 'cite', 'interview', 'specify', 'clarify', 'liaise', 'speak',\n",
    "                      'commend', 'listen', 'suggest', 'communicate', 'log', 'synthesize', 'compile', 'mediate', 'telegraph', \n",
    "                      'consult', 'narrate', 'transcribe', 'contact', 'notate', 'note', 'translate', 'convey', 'notify', 'transmit',\n",
    "                      'convince', 'outline', 'tweet', 'correspond', 'pen', 'verbalize', 'source', 'portray', 'write', 'debate',\n",
    "                      'post', 'define', 'present', 'detail', 'describe', 'proclaim', 'protest', 'demonstrate', 'illustrate']\n",
    "\n",
    "Communications = []\n",
    "j = 0\n",
    "for doc in Quotes:\n",
    "    print(j)\n",
    "    j+=1\n",
    "    if doc[1] == None:\n",
    "        Communications.append(None)\n",
    "    else:\n",
    "        Communications.append(len([tok for sent in doc[1] for tok in sent[0] if tok[\"upos\"] == 'VERB' and tok[\"lemma\"] in communicationVerbs])/len(doc[1])) #sent[0] unquoted parts of sent\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Modals = [] #Average number of modals verbs outside of quotes per sentence\n",
    "j = 0\n",
    "for doc in Quotes:\n",
    "    print(j)\n",
    "    j+=1\n",
    "    if doc[1] == None:\n",
    "        Modals.append(None)\n",
    "    else:\n",
    "        Modals.append(len([tok for sent in doc[1] for tok in sent[0] if tok[\"xpos\"] == 'MD'])/len(doc[1])) #sent[0] unquoted parts of sent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of future verbs with auxiliary \"will\"\n",
    "\n",
    "Wills = []\n",
    "j = 0\n",
    "for doc in Quotes:\n",
    "    print(j)\n",
    "    j+=1\n",
    "    if doc[1] == None:\n",
    "        Wills.append(None)\n",
    "    else:\n",
    "        Wills.append(len([tok for sent in doc[1] for tok in sent[0] if tok[\"upos\"] == 'AUX' and tok[\"text\"] == 'will'])/len(doc[1])) #sent[0] unquoted parts of sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FirstPron(tok):\n",
    "    # Returns whether token is a first person pronoun\n",
    "    fts = feats(tok)\n",
    "    return 'Person' in fts and fts['Person'] == \"1\"\n",
    "\n",
    "Firsts = []\n",
    "j = 0\n",
    "for doc in Quotes:\n",
    "    print(j)\n",
    "    j+=1\n",
    "    if doc[1] == None:\n",
    "        Firsts.append(None)\n",
    "    else:\n",
    "        Firsts.append(len([tok for sent in doc[1] for tok in sent[0] if FirstPron(tok)])/sum([len(sent) for sent in doc[1]])) #sent[0] unquoted parts of sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SecondPron(tok):\n",
    "    # Returns whether token is a second person pronoun\n",
    "    fts = feats(tok)\n",
    "    return 'Person' in fts and fts['Person'] == \"2\"\n",
    "\n",
    "Seconds = []\n",
    "j = 0\n",
    "for doc in Quotes:\n",
    "    print(j)\n",
    "    j+=1\n",
    "    if doc[1] == None:\n",
    "        Seconds.append(None)\n",
    "    else:\n",
    "        Seconds.append(len([tok for sent in doc[1] for tok in sent[0] if SecondPron(tok)])/sum([len(sent) for sent in doc[1]])) #sent[0] unquoted parts of sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NumDigits(numString):\n",
    "    n = 0\n",
    "    for c in numString:\n",
    "        if c.isdigit():\n",
    "            n += 1\n",
    "    return n\n",
    "\n",
    "Digits = []\n",
    "j = 0\n",
    "for doc in StanzaDicts:\n",
    "    print(j)\n",
    "    j+=1\n",
    "    if doc[1] == None:\n",
    "        Digits.append(None)\n",
    "    else:\n",
    "        Digits.append(sum([NumDigits(tok[\"text\"]) for sent in doc[1] for tok in sent if tok[\"upos\"] == 'NUM' or tok[\"xpos\"] == \"CD\"])/sum([len(sent) for sent in doc[1]])) #sent[0] unquoted parts of sent\n",
    "\n",
    "Nums = [] # Average frequency of numbers in digits only occuring per token\n",
    "j = 0\n",
    "for doc in StanzaDicts:\n",
    "    print(j)\n",
    "    j+=1\n",
    "    if doc[1] == None:\n",
    "        Nums.append(None)\n",
    "    else:\n",
    "        Nums.append(len([tok for sent in doc[1] for tok in sent if (tok[\"upos\"] == 'NUM' or tok[\"xpos\"] == \"CD\") and tok[\"text\"].isdigit()])/sum([len(sent) for sent in doc[1]])) #sent[0] unquoted parts of sent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Inters = [] # Average frequency of interjections occuring per token\n",
    "j = 0\n",
    "for doc in StanzaDicts:\n",
    "    print(j)\n",
    "    j+=1\n",
    "    if doc[1] == None:\n",
    "        Inters.append(None)\n",
    "    else:\n",
    "        Inters.append(len([tok for sent in doc[1] for tok in sent if tok[\"upos\"] == 'INTJ' or tok[\"xpos\"] == \"UH\"])/sum([len(sent) for sent in doc[1]])) #sent[0] unquoted parts of sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SUBJLexicon():\n",
    "    \n",
    "    # Builds subjectivity lexicon from MPQA clues file\n",
    "    \n",
    "    lex = {}\n",
    "    f = open(\"subjclueslen1-HLTEMNLP05.tff\", \"r\")\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        params = line.split()\n",
    "        word = params[2][6:]\n",
    "        pos = params[3][5:]\n",
    "        strength = {'weaksubj':.1, 'strongsubj':1}[params[0][5:]]\n",
    "        lex[word] = {'pos':pos, 'subjectivity':strength}\n",
    "    return lex\n",
    "    \n",
    "\n",
    "lexicon = SUBJLexicon()\n",
    "\n",
    "def lemmatize(word):\n",
    "    \n",
    "    # lemmatizes a single word\n",
    "    \n",
    "    token = nlp(word).sentences[0].tokens[0].to_dict()[0]\n",
    "    return token[\"lemma\"]\n",
    "\n",
    "def inverselem(lexicon):\n",
    "    \n",
    "    # Creates a lexicon for the lemmas of those appearing in the first lexicon\n",
    "    \n",
    "    lex = {}\n",
    "    j = 0\n",
    "    for k, v in lexicon.items():\n",
    "        print(j)\n",
    "        j += 1\n",
    "        lex[lemmatize(k)] = v\n",
    "    return lex\n",
    "\n",
    "invlex = inverselem(lexicon)\n",
    "\n",
    "def subjectivity(token, lexicon, invlex):\n",
    "    \n",
    "    # Returns the 'subjectivity' of a token w.r.t. the MPQA lexicon as described in Krüger et a\n",
    "    \n",
    "    if token[\"text\"] in lexicon:\n",
    "        return lexicon[token[\"text\"]][\"subjectivity\"]\n",
    "    elif token[\"lemma\"] in lexicon:\n",
    "        return lexicon[token[\"lemma\"]][\"subjectivity\"]\n",
    "    elif token[\"lemma\"] in invlex:\n",
    "        return invlex[token[\"lemma\"]][\"subjectivity\"]\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "        \n",
    "Subjectivities = []\n",
    "j = 0\n",
    "for doc in Quotes:\n",
    "    print(j)\n",
    "    j+=1\n",
    "    if doc[1] == None:\n",
    "        Subjectivities.append(None)\n",
    "    else:\n",
    "        Subjectivities.append(sum([subjectivity(tok, lexicon, invlex) for sent in doc[1] for tok in sent[0]])/sum([len(sent) for sent in doc[1]])) #sent[0] unquoted parts of sent\n",
    "\n",
    "        \n",
    "AdjSubjectivities = []\n",
    "j = 0\n",
    "for doc in Quotes:\n",
    "    print(j)\n",
    "    j+=1\n",
    "    if doc[1] == None:\n",
    "        AdjSubjectivities.append(None)\n",
    "    else:\n",
    "        AdjSubjectivities.append(sum([subjectivity(tok, lexicon, invlex) for sent in doc[1] for tok in sent[0] if tok[\"upos\"] == 'ADJ'])/sum([len(sent) for sent in doc[1]])) #sent[0] unquoted parts of sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AvgSentLengths = []\n",
    "j = 0\n",
    "for doc in StanzaDicts:\n",
    "    print(j)\n",
    "    j += 1\n",
    "    if doc[1] == None:\n",
    "        AvgSentLengths.append(None)\n",
    "    else:\n",
    "        AvgSentLengths.append(sum([len(sent) for sent in doc[1]])/len(doc[1]))\n",
    "        \n",
    "AvgTokLengths = []\n",
    "j = 0\n",
    "for doc in StanzaDicts:\n",
    "    print(j)\n",
    "    j += 1\n",
    "    if doc[1] == None:\n",
    "        AvgTokLengths.append(None)\n",
    "    else:\n",
    "        AvgTokLengths.append(sum([len(tok[\"text\"]) for sent in doc[1] for tok in sent])/sum([len(sent) for sent in doc[1]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we list all the linguistic features for each document together in one list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LingFeats = []\n",
    "j = 0\n",
    "for doc in StanzaDicts:\n",
    "    print(j)\n",
    "    j += 1\n",
    "    if doc[1] == None:\n",
    "        LingFeats.append((doc[0], None))\n",
    "    else:\n",
    "        dictionary = {}\n",
    "        dictionary[\"AvgSentLengths\"] = 1/AvgSentLengths[j-1]\n",
    "        dictionary[\"AvgTokLengths\"] = 1/AvgTokLengths[j-1]\n",
    "        dictionary[\"NormNegation\"] = NormNegation[j-1]\n",
    "        dictionary[\"NegationRatio\"] = NegationRatio[j-1]\n",
    "        dictionary[\"NormNegSuffix\"] = NormNegSuffix[j-1]\n",
    "        dictionary[\"NegSuffixRatio\"] = NegSuffixRatio[j-1]\n",
    "        dictionary[\"Complexity\"] = Complexity[j-1]\n",
    "        dictionary[\"Questions\"] = Questions[j-1]\n",
    "        dictionary[\"Semicolons\"] = Semicolons[j-1]\n",
    "        dictionary[\"Commas\"] = Commas[j-1]\n",
    "        dictionary[\"Causals\"] = Causals[j-1]\n",
    "        dictionary[\"Temporals\"] = Temporals[j-1]\n",
    "        dictionary[\"Contrastives\"] = Contrastives[j-1]\n",
    "        dictionary[\"Expansives\"] = Expansives[j-1]\n",
    "        dictionary[\"Connectives\"] = Connectives[j-1]\n",
    "        dictionary[\"Citations\"] = Citations[j-1]\n",
    "        dictionary[\"CitationLengths\"] = CitationLengths[j-1]\n",
    "        dictionary[\"Pasts\"] = Pasts[j-1]\n",
    "        dictionary[\"Presents\"] = Presents[j-1]\n",
    "        dictionary[\"Modals\"] = Modals[j-1]\n",
    "        dictionary[\"Wills\"] = Wills[j-1]\n",
    "        dictionary[\"Firsts\"] = Firsts[j-1]\n",
    "        dictionary[\"Seconds\"] = Seconds[j-1]\n",
    "        dictionary[\"Digits\"] = Digits[j-1]\n",
    "        dictionary[\"Nums\"] = Nums[j-1]\n",
    "        dictionary[\"Inters\"] = Inters[j-1]\n",
    "        dictionary[\"Subjectivities\"] = Subjectivities[j-1]\n",
    "        dictionary[\"AdjSubjectivities\"] = AdjSubjectivities[j-1]\n",
    "        LingFeats.append((doc[0], dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"LingFeats.pckl\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(LingFeats, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "News = [d for d in categorized if d[1] == (True, False)]\n",
    "Opinion = [d for d in categorized if d[1] == (False, True)]\n",
    "Both = [d for d in categorized if d[1] == (True, True)]\n",
    "Neither = [d for d in categorized if d[1] == (False, False)]\n",
    "\n",
    "print(len(News), len(Opinion), len(Both), len(Neither))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First we evaluate a linear SVM using only lemma frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "cv = CountVectorizer(max_df=0.95, min_df=0.002,max_features=2000,stop_words='english')\n",
    "transformer = TfidfTransformer(use_idf = False)\n",
    "tfidf = TfidfVectorizer(max_df=0.95, min_df=0.002, max_features=2000, stop_words='english')\n",
    "\n",
    "CorpusWordCounts = cv.fit_transform([t[1] for t in Texts])\n",
    "CVWordIndex = cv.get_feature_names()\n",
    "TFMatrix = transformer.transform(CorpusWordCounts)\n",
    "TFidfMatrix = tfidf.fit_transform([t[1] for t in Texts])\n",
    "IDFWordIndex = tfidf.get_feature_names()\n",
    "\n",
    "print(CVWordIndex == IDFWordIndex)\n",
    "\n",
    "with open(\"TFMatrix.pckl\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(TFMatrix, fp)\n",
    "with open(\"TFidfMatrix.pckl\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(TFidfMatrix, fp)\n",
    "with open(\"CVWordIndex.pckl\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(CVWordIndex, fp)\n",
    "with open(\"IDFWordIndex.pckl\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(IDFWordIndex, fp)\n",
    "    \n",
    "defects = [i for i in range(len(StanzaDicts)) if StanzaDicts[i][1] == None]\n",
    "\n",
    "Indices = [i for i in range(len(StanzaDicts)) if StanzaDicts[i][1] != None and sum(categorized[i][1]) == 1]\n",
    "\n",
    "y = [int(cat[1][1]) for cat in categorized[Indices]]\n",
    "X = TFidfMatrix[Indices]\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "clf = make_pipeline(StandardScaler(with_mean=False),LinearSVC(random_state=0, tol=1e-5))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=433)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(clf.score(X_test, y_test)) # Got about 97-98 percent accuracy here\n",
    "\n",
    "X = TFMatrix[Indices]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=33)\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.score(X_test, y_test)) # Around 97 percent accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now a linear SVM with only linguistic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LingKeys = LingFeats[0][1].keys()\n",
    "LingKeys = list(LingKeys)\n",
    "ind2feat = {i : LingKeys[i] for i in range(len(LingKeys))}\n",
    "feat2ind = {v : k for k,v in ind2feat.items()}\n",
    "\n",
    "LINGmatrix = []\n",
    "j = 0\n",
    "for doc in LingFeats:\n",
    "    print(j)\n",
    "    j += 1\n",
    "    featsArray = []\n",
    "    if doc[1] == None:\n",
    "        featsArray = None\n",
    "    else:\n",
    "        for k in ind2feat.keys():\n",
    "            featsArray.append(doc[1][ind2feat[k]])\n",
    "    LINGmatrix.append(featsArray)\n",
    "    \n",
    "with open(\"LINGmatrix.pckl\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(LINGmatrix, fp)\n",
    "    \n",
    "LINGmatrix = np.asarray(LINGmatrix)\n",
    "\n",
    "X = LINGmatrix[Indices]\n",
    "\n",
    "rows = X.shape[0]\n",
    "\n",
    "flatX = np.concatenate(X)\n",
    "\n",
    "X = flatX.reshape(rows, -1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=33)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.score(X_test, y_test)) # Around 94-95 percent accuracy obtained here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate linear SVM using upos, xpos, and combination of both tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uposTags = set([tok[\"upos\"] for doc in [doc for doc in StanzaDicts if doc[1] != None] for sent in doc[1] for tok in sent])\n",
    "\n",
    "def emptyUposDict():\n",
    "    \n",
    "    #Initialize dictionary of upos counts\n",
    "    \n",
    "    d = {}\n",
    "    for tag in uposTags:\n",
    "        d[tag] = 0\n",
    "    return d\n",
    "\n",
    "UPOScounts = []\n",
    "j = 0\n",
    "for doc in StanzaDicts:\n",
    "    print(j)\n",
    "    j += 1\n",
    "    docUpos = None\n",
    "    if doc[1] != None:\n",
    "        docUpos = emptyUposDict()\n",
    "        for sent in doc[1]:\n",
    "            for tok in sent:\n",
    "                docUpos[tok[\"upos\"]] += 1\n",
    "    UPOScounts.append(docUpos)\n",
    "\n",
    "xposTags = set([tok[\"xpos\"] for doc in [doc for doc in StanzaDicts if doc[1] != None] for sent in doc[1] for tok in sent])\n",
    "\n",
    "def emptyXposDict():\n",
    "    \n",
    "    #Initialize dictionary of xpos counts\n",
    "    \n",
    "    d = {}\n",
    "    for tag in xposTags:\n",
    "        d[tag] = 0\n",
    "    return d\n",
    "\n",
    "XPOScounts = []\n",
    "j = 0\n",
    "for doc in StanzaDicts:\n",
    "    print(j)\n",
    "    j += 1\n",
    "    docXpos = None\n",
    "    if doc[1] != None:\n",
    "        docXpos = emptyXposDict()\n",
    "        for sent in doc[1]:\n",
    "            for tok in sent:\n",
    "                docXpos[tok[\"xpos\"]] += 1\n",
    "    XPOScounts.append(docXpos)\n",
    "    \n",
    "\n",
    "with open(\"UPOScounts.pckl\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(UPOScounts, fp)\n",
    "with open(\"XPOScounts.pckl\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(XPOScounts, fp)\n",
    "    \n",
    "def normalize(dic):\n",
    "    N = sum(dic.values())\n",
    "    return {k:v/N for k,v in dic.items()}\n",
    "\n",
    "UPOSfreqs = []\n",
    "for i in range(len(UPOScounts)):\n",
    "    print(i)\n",
    "    if UPOScounts[i] != None:\n",
    "        UPOSfreqs.append(normalize(UPOScounts[i]))\n",
    "    else:\n",
    "        UPOSfreqs.append(None)\n",
    "        \n",
    "XPOSfreqs = []\n",
    "for i in range(len(XPOScounts)):\n",
    "    print(i)\n",
    "    if XPOScounts[i] != None:\n",
    "        XPOSfreqs.append(normalize(XPOScounts[i]))\n",
    "    else:\n",
    "        XPOSfreqs.append(None)\n",
    "        \n",
    "with open(\"UPOSfreqs.pckl\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(UPOSfreqs, fp)\n",
    "with open(\"XPOSfreqs.pckl\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(XPOSfreqs, fp)\n",
    "    \n",
    "uposKeys = list(UPOSfreqs[0].keys())\n",
    "ind2upos = {i:uposKeys[i] for i in range(len(uposKeys))}\n",
    "upos2ind = {v:k for k,v in ind2upos.items()}\n",
    "\n",
    "UPOSmatrix = []\n",
    "j = 0\n",
    "for doc in UPOSfreqs:\n",
    "    print(j)\n",
    "    j += 1\n",
    "    if doc == None:\n",
    "        UPOSmatrix.append(None)\n",
    "    else:\n",
    "        UPOSmatrix.append([doc[ind2upos[i]] for i in range(len(ind2upos))])\n",
    "        \n",
    "UPOSmatrix = np.asarray(UPOSmatrix)\n",
    "\n",
    "UPOSmatrix = UPOSmatrix[Indices]\n",
    "\n",
    "print(rows == UPOSmatrix.shape[0])\n",
    "\n",
    "flatUPOS = np.concatenate(UPOSmatrix) #flatten list of lists\n",
    "\n",
    "UPOSmatrix = flatUPOS.reshape(rows, -1)\n",
    "\n",
    "X = UPOSmatrix\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=33)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test) # About 94 percent accuracy\n",
    "\n",
    "xposKeys = list(XPOSfreqs[0].keys())\n",
    "ind2xpos = {i:xposKeys[i] for i in range(len(xposKeys))}\n",
    "xpos2ind = {v:k for k,v in ind2xpos.items()}\n",
    "\n",
    "XPOSmatrix = []\n",
    "j = 0\n",
    "for doc in XPOSfreqs:\n",
    "    print(j)\n",
    "    j += 1\n",
    "    if doc == None:\n",
    "        XPOSmatrix.append(None)\n",
    "    else:\n",
    "        XPOSmatrix.append([doc[ind2xpos[i]] for i in range(len(ind2xpos))])\n",
    "        \n",
    "XPOSmatrix = np.asarray(XPOSmatrix)\n",
    "\n",
    "XPOSmatrix = XPOSmatrix[Indices]\n",
    "\n",
    "print(rows == XPOSmatrix.shape[0])\n",
    "\n",
    "flatXPOS = np.concatenate(XPOSmatrix)\n",
    "\n",
    "XPOSmatrix = flatXPOS.reshape(rows, -1)\n",
    "\n",
    "X = XPOSmatrix\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=33)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test) # About 95 percent accuracy\n",
    "\n",
    "\n",
    "# Now try combining upos and xpos tags\n",
    "\n",
    "\n",
    "X = np.hstack((UPOSmatrix, XPOSmatrix))\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=33)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test) # Slightly less than 95 percent accuracy. Worse than xpos alone! However, not by a significant amount\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear SVM with lemma and pos tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LINGmatrix = LINGmatrix[Indices]\n",
    "flatLING = np.concatenate(LINGmatrix)\n",
    "print(rows == LINGmatrix.shape[0])\n",
    "LINGmatrix = flatLING.reshape(rows, -1)\n",
    "X = np.hstack((LINGmatrix, UPOSmatrix, XPOSmatrix))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=33)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test) # About 96 percent accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear SVM with linguist features, lemma frequencies and pos tag frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix, hstack\n",
    "\n",
    "X = hstack((TFIDFmat, csr_matrix(X)))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=33)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test) # About 97 percent accuracy. Not as good as tdidf alone!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
